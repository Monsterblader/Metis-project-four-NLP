{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autotime"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# time: 1.88 s\n","\n","import psycopg2 as pg\n","import pandas as pd\n","import pickle\n","import numpy\n","import os\n","import spacy\n","import string\n","import time\n","import lib.functions\n","\n","# import textacy\n","from spacy import attrs\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","from sklearn.base import TransformerMixin\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# time: 1min 51s\n","\n","texts = {}\n","texts['wsb'] = pd.read_json('archive/wallstreetbets_submission.json', lines=True)\n","texts['insub'] = pd.read_json('archive/investing_submission.json', lines=True)\n","texts['opsub'] = pd.read_json('archive/options_submission.json', lines=True)\n","texts['secan'] = pd.read_json('archive/SecurityAnalysis_submission.json', lines=True)\n","\n","all_text = pd.DataFrame()\n","all_text = all_text.append(texts['insub'])\n","all_text = all_text.append(texts['wsb'][-150000:])\n","all_text = all_text.append(texts['opsub'])\n","all_text = all_text.append(texts['secan'])\n","\n","all_text['selftext'] = all_text['selftext'].apply(lambda x: x.strip() if type(x) == str else x)\n","all_text = all_text[all_text['selftext'] != \"\"]\n","all_text = all_text[all_text['selftext'] != \"[removed]\"]\n","all_text = all_text[all_text['selftext'] != \"[deleted]\"]\n","all_text = all_text[all_text['selftext'] != '.']\n","all_text = all_text[pd.notna(all_text['selftext'])]\n","all_text.reset_index(drop=True, inplace=True)\n","\n","all_text.to_csv('archive/sub_reddits.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# time: 210 ms\n","\n","sorttext = sorted(all_text['selftext'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Postgres info to connect\n","connection_args = {\n","    'host': 'localhost',  # We are connecting to our _local_ version of psql\n","    'dbname': 'WSB',    # DB that we are connecting to\n","    'port': 5432          # port we opened on AWS\n","}\n","\n","connection = pg.connect(**connection_args)  # What is that \"**\" there??\n","cur = connection.cursor()\n","\n","sqlCreateTable = \"create table raw_data (id bigint, type varchar(4), posts text);\"\n","\n","cur.execute(sqlCreateTable)\n","connection.commit()\n","\n","# Didn't work.  Created manually.\n","\n","\n","def make_SQL_string():\n","    columns = [\"post_\" + str(i) + \" text, \" for i in range(58)]\n","\n","    return \"CREATE TABLE posts;\"\n","\n","\n","cur.execute(make_SQL_string())\n","connection.commit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["churchill = pd.read_csv('sources/Churchill/Churchill.csv', sep=\"|\", header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# time: 4.31 s\n","\n","all_text = pd.read_csv('archive/sub_reddits.csv')\n","all_text['selftext'] = all_text['selftext'].astype(str)"]},{"source":["# Needs additional cleaning"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["docs = [doc for doc in nlp.pipe(all_text['selftext'][:2], batch_size=250, n_threads=4)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('archive/text_1.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[0:100000], picklefile)\n","with open('archive/text_2.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[100000:200000], picklefile)\n","with open('archive/text_3.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[200000:300000], picklefile)\n","with open('archive/text_4.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[300000:344748], picklefile)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('archive/text_1.pkl', 'rb') as picklefile:\n","    all_text = pickle.load(picklefile)\n","with open('archive/text_2.pkl', 'rb') as picklefile:\n","    text2 = pickle.load(picklefile)\n","with open('archive/text_3.pkl', 'rb') as picklefile:\n","    text3 = pickle.load(picklefile)\n","with open('archive/text_4.pkl', 'rb') as picklefile:\n","    text4 = pickle.load(picklefile)\n","\n","all_text.extend(text2)\n","all_text.extend(text3)\n","all_text.extend(text4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bow_reprs = [doc.count_by(attrs.LOWER) for doc in all_text]\n","\n","vocab_keys = set()\n","for bow_rep in bow_reprs:\n","    vocab_keys.update(set(bow_rep.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pos_reprs = [doc.count_by(attrs.POS) for doc in all_text]\n","pos_keys = set()\n","for pos_rep in pos_reprs:\n","    pos_keys.update(set(pos_rep.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lexeme_filter(lexeme):\n","    if lexeme.is_digit:\n","        return(False)\n","    if lexeme.is_punct:\n","        return(False)\n","    if lexeme.is_space:\n","        return(False)\n","    if lexeme.like_num:\n","        return(False)\n","    if lexeme.like_email:\n","        return(False)\n","    return(True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_lexemes = [nlp.vocab[vk] for vk in vocab_keys]\n","vocab_lexemes_filtered = [vl for vl in vocab_lexemes if lexeme_filter(vl)]\n","\n","lexeme_encoding = {lexeme.lower : i for i,lexeme in enumerate(vocab_lexemes_filtered)}\n","rev_lexeme_encoding = {i:k for k,i in lexeme_encoding.items()}\n","lexeme_word_lookup = {lexeme.lower : lexeme.lower_ for lexeme in vocab_lexemes_filtered}\n","n_words = len(lexeme_encoding)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lexeme_lower_bow_to_vec(lexeme_lower_bow, lexeme_encoding):\n","    bow_vec = numpy.zeros((len(lexeme_encoding,)), dtype=numpy.int64)\n","    for k,v in lexeme_lower_bow.items():\n","        try:\n","            bow_vec[lexeme_encoding[k]] += v\n","        except KeyError:\n","            pass\n","    return(bow_vec)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm = numpy.vstack(lexeme_lower_bow_to_vec(bow_rep, lexeme_encoding)\n","                   for bow_rep in bow_reprs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_df = pd.DataFrame([key for key in lexeme_encoding])\n","lexeme_df.columns = ['key']\n","lexeme_df['col_headers'] = lexeme_df['key'].map(lexeme_encoding)\n","lexeme_df['key_words'] = lexeme_df['key'].map(lexeme_word_lookup)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_df = pd.DataFrame([key for key in lexeme_encoding])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm_df = pd.DataFrame(tdm)\n","tdm_df.columns = lexeme_df['key_words']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"orig_nbformat":2,"kernelspec":{"name":"Python 3.8.5 64-bit ('metis': conda)","display_name":"Python 3.8.5 64-bit ('metis': conda)","metadata":{"interpreter":{"hash":"1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"}}}}}