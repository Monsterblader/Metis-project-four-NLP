{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 5.07 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tools.functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 241 µs\n"
     ]
    }
   ],
   "source": [
    "vectorizer_max_features = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 6.32 s\n"
     ]
    }
   ],
   "source": [
    "all_text = pd.read_csv('archive/sub_reddits.csv')\n",
    "all_text['selftext'] = all_text['selftext'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "sanitized_posts_df = pd.DataFrame(tools.functions.sanitize_posts(all_text['selftext']), columns=['posts'])\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != 'a']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '_']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'].str.contains('[^\\d^\\s]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 703 µs\n"
     ]
    }
   ],
   "source": [
    "# Workspace for additional text cleaning"
   ]
  },
  {
   "source": [
    "# This section was used for running TF-IDF on the corpus.  See where/if it fits."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was this wrong?  I should only be running vectorizer.fit?\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "\n",
    "tfidfvectorized_posts = vectorizer.fit_transform(sanitized_posts_df['posts'])\n",
    ".toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "tfidf = vectorizer.fit(sanitized_posts_df['posts'])\n",
    "tfidf_transformed = vectorizer.transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 22 s\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(tfidf_transformed.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    00      000  0000  00000  000000  000001  00001  0001  0001104659  \\\n",
       "0  0.0  0.00000   0.0    0.0     0.0     0.0    0.0   0.0         0.0   \n",
       "1  0.0  0.12075   0.0    0.0     0.0     0.0    0.0   0.0         0.0   \n",
       "2  0.0  0.00000   0.0    0.0     0.0     0.0    0.0   0.0         0.0   \n",
       "3  0.0  0.00000   0.0    0.0     0.0     0.0    0.0   0.0         0.0   \n",
       "4  0.0  0.00000   0.0    0.0     0.0     0.0    0.0   0.0         0.0   \n",
       "\n",
       "   0001193125  ...   то  уоu  что  это  ісо  الاستثمار   في  كما   من  \\\n",
       "0         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   \n",
       "1         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   \n",
       "2         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   \n",
       "3         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   \n",
       "4         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   \n",
       "\n",
       "   ﬁnancial  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "\n",
       "[5 rows x 44890 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0000</th>\n      <th>00000</th>\n      <th>000000</th>\n      <th>000001</th>\n      <th>00001</th>\n      <th>0001</th>\n      <th>0001104659</th>\n      <th>0001193125</th>\n      <th>...</th>\n      <th>то</th>\n      <th>уоu</th>\n      <th>что</th>\n      <th>это</th>\n      <th>ісо</th>\n      <th>الاستثمار</th>\n      <th>في</th>\n      <th>كما</th>\n      <th>من</th>\n      <th>ﬁnancial</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.12075</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 44890 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 60.5 ms\n"
     ]
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "source": [
    "# The following section runs a Latent Semantic Analysis (LSA) on the corpus using TruncatedSVD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "count_vectorized_posts = count_vectorizer.fit_transform(sanitized_posts_df['posts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.13418152 0.11829433]\n0.25247584468078527\n[3170.21868301 2869.74127917]\ntime: 5 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "svd_fit_transform = svd.fit_transform(count_vectorized_posts)\n",
    "\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                00    000  0000  00000  000000  000001  00001  0001  \\\n",
       "component_1  0.037  0.024   0.0    0.0     0.0     0.0    0.0   0.0   \n",
       "component_2 -0.012 -0.005  -0.0   -0.0    -0.0    -0.0   -0.0  -0.0   \n",
       "\n",
       "             0001104659  0001193125  ...   то  уоu  что  это  ісо  الاستثمار  \\\n",
       "component_1         0.0         0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0   \n",
       "component_2        -0.0        -0.0  ... -0.0 -0.0 -0.0 -0.0 -0.0       -0.0   \n",
       "\n",
       "              في  كما   من  ﬁnancial  \n",
       "component_1  0.0  0.0  0.0       0.0  \n",
       "component_2 -0.0 -0.0 -0.0      -0.0  \n",
       "\n",
       "[2 rows x 44890 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0000</th>\n      <th>00000</th>\n      <th>000000</th>\n      <th>000001</th>\n      <th>00001</th>\n      <th>0001</th>\n      <th>0001104659</th>\n      <th>0001193125</th>\n      <th>...</th>\n      <th>то</th>\n      <th>уоu</th>\n      <th>что</th>\n      <th>это</th>\n      <th>ісо</th>\n      <th>الاستثمار</th>\n      <th>في</th>\n      <th>كما</th>\n      <th>من</th>\n      <th>ﬁnancial</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.037</td>\n      <td>0.024</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>-0.012</td>\n      <td>-0.005</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 44890 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 63.8 ms\n"
     ]
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(svd.components_.round(3), index=[\"component_1\", \"component_2\"], columns=vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 478 µs\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\namp, http, com, gt, earnings\n\nTopic  1\ncomment, wallstreetbets, 17, 15, spy\ntime: 37.7 ms\n"
     ]
    }
   ],
   "source": [
    "display_topics(svd, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    component_1  component_2\n",
       "posts                                                                       \n",
       "so m sober a judge about to go to sleep when ha...      1.14076     -0.32376\n",
       "i just sold my first house project house that v...      0.88391     -0.18157\n",
       "we ve all heard the story about those numerous ...      4.34081     -1.40833\n",
       "i want to learn about all type of investing the...      0.42224     -0.12648\n",
       "can someone recommend good charting site where ...      1.09618     -0.32689\n",
       "...                                                         ...          ...\n",
       "disclaimer am long equity please do due diligen...     12.45172     -3.36845\n",
       "a few month ago ran across forum very similar t...      0.37083     -0.09373\n",
       "love researching quality information about inte...      0.97487     -0.31669\n",
       "knowing everything you know now a an investor i...      0.61824      0.53665\n",
       "i contacted lead sell side analyst to discus sm...      0.84630     -0.26575\n",
       "\n",
       "[344358 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>component_1</th>\n      <th>component_2</th>\n    </tr>\n    <tr>\n      <th>posts</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>so m sober a judge about to go to sleep when have this idea and get up and go write it down in notepad because that a good idea then get better one which is to submit it here so here is what wrote expected variance over time option put and call price are they linear function with respect to time maximize call and putt cost v gross rev combined with prob dist of stock price profit maybe if we get something going we can submit it to programing and get working program use subreddits like department lol</th>\n      <td>1.14076</td>\n      <td>-0.32376</td>\n    </tr>\n    <tr>\n      <th>i just sold my first house project house that ve lived in while renovating part time for the past 3 year and have 15 000 in profit that need to invest the money will be used a part of down payment on my next house in roughly 2 5 year have couple of idea but would like to hear what you would do in this situation edit what about etf a large portion of this investment pro con personal thought favorite fund etc</th>\n      <td>0.88391</td>\n      <td>-0.18157</td>\n    </tr>\n    <tr>\n      <th>we ve all heard the story about those numerous non penny stock that sold at penny level on thursday to me it pretty obvious what happened the s government must have instructed some of the largest volume trader institutional trader to dump large volume of those stock at penny level right at 2 38pm eastern time on thursday in an orchestrated effort to drive down key s stock market index and thereby dramatically devalue the s stock market objection 1 wouldn those institution lose combined billion of dollar by dumping those stock at penny level yes indeed they would but believe that the s federal reserve promised to recoup those institution whatever loss they incurred objection 2 it no secret that china among other country ha been trying to slow down it economy to stop t currency from being too quickly devalued is it so hard to imagine then that the s would want to do the same thing only more covertly the s dollar ha been severely devalued recently until this week that is one ha to wonder which firm gained from all of of these s stock being sold off at penny level suspect they could have been chinese investor in other word it very possible that china just took gigantic windfall profit at the expense of the s stock market precisely when they didn want to take one did the s market manipulation capability just assert it dominance over chinese market manipulation capability know it seems counter intuitive but so doe the idea of slowing down your own economy in order to bolster your currency would not be at all surprised to see this currency warfare continue for month until the big chinese firm learn how to fight back thursday just proved that american innovation is leap and bound ahead of the chinese in regard to market and currency manipulation the dollar is ridiculously stronger today than it wa only few day ago against both the euro and the yuan</th>\n      <td>4.34081</td>\n      <td>-1.40833</td>\n    </tr>\n    <tr>\n      <th>i want to learn about all type of investing then may decide to focus and study on certain area more specifically will shortly have lot more time on my hand and love to read and study new thing doe anyone have any resource they can share be it book to buy online literature etc</th>\n      <td>0.42224</td>\n      <td>-0.12648</td>\n    </tr>\n    <tr>\n      <th>can someone recommend good charting site where can see all of my stock in big long list with each displaying one year chart option for 2 3 5 would be nice preferably with moving average rsi etc a it is now use yahoo finance and clicking through to each stock 30 just take way too long</th>\n      <td>1.09618</td>\n      <td>-0.32689</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>disclaimer am long equity please do due diligence this is based off of 60 minute of quick analysis company overview xeris is spec pharma company founded in 2005 their primary scope of work involves developed injectable and infusible drug 2019 present product launch their first product wa approved in september 2019 called gvoke it is pfs and auto injector that ha glucagon to treat severe hypoglycemia this is market a two different product gvoke pfs nov 2019 and gvoke hypopen july 2020 competition primary competition come from tradition glucagon kit and eli lilly baqsimi baqsimi is delivered via the nasal passage legacy kit are traditional syringe injection xeris ha the advantage with both pfs and auto injector which traditionally are well received with patient amp x200b baqsimi http preview redd it ddtlxc9revg51 jpg width 960 amp format pjpg amp auto webp amp b046cc1e8c8253d0a7ebed32ae36fa61308834a6 amp x200b legacy kit http preview redd it pachhzbuevg51 jpg width 350 amp format pjpg amp auto webp amp 77df31e4c44ddb60207ebaf5f6103958ddcbd8b4 amp x200b financials the most popular product will most likely be the two pack hypopen auto injector this carry awp of 673 92 for adult the prescription is 1mg 0 2ml baqsimi cost is similar with 3 mg dosage and legacy kit from lly cost 280 kit amp x200b gvoke micromedex http preview redd it i5qo0ukvevg51 jpg width 717 amp format pjpg amp auto webp amp 953a76cddb98d17ec991e9a742fcbdc0db05d967 one overhang with xeris financials is their long term debt which ha increased from 58 3m ye19 to 109 5m 2q20 however principal payment do not start until 2022 and interest expense should be below 10m for the year xeris should be generating enough fcf over the next two year to service their debt valuation takeaway peak sale of 250m wacc 11 positive ebitda by 2026 amp x200b dcf http preview redd it 10k7gz5mevg51 jpg width 1304 amp format pjpg amp auto webp amp 263486dcc4c7fc9bb015888a646038b97f0a1ab8 amp x200b price target http preview redd it 22e13qipevg51 jpg width 423 amp format pjpg amp auto webp amp 3b906a275c204f3e3ccde14d2f896e9b891198e0</th>\n      <td>12.45172</td>\n      <td>-3.36845</td>\n    </tr>\n    <tr>\n      <th>a few month ago ran across forum very similar to reddit where people could anonymously publish short thesis unfortunately ve been unable to find it since doe anyone know if it still up or know what m talking about</th>\n      <td>0.37083</td>\n      <td>-0.09373</td>\n    </tr>\n    <tr>\n      <th>love researching quality information about interesting company however it is hard to find those at the intersection of intriguing yet understandable to an outsider this unfortunately rule out most of pharma for example ve really enjoyed following tesla a ve always been passionate about alternative source of energy and low cost airline a ve been flying around europe since wa only few month old love ryanair and wizz though haven actually invested in any of those two but in u low cost airline instead what interesting to note is that usually the more engaging the company the better it ha done for me financially looking forward to your tip</th>\n      <td>0.97487</td>\n      <td>-0.31669</td>\n    </tr>\n    <tr>\n      <th>knowing everything you know now a an investor if you could go back in time and give your beginner self advice what would it be share yours in the comment let spread wisdom for everyone</th>\n      <td>0.61824</td>\n      <td>0.53665</td>\n    </tr>\n    <tr>\n      <th>i contacted lead sell side analyst to discus small cap had been researching at beginning of quarter we spoke on the phone my favorite fund initiated it largest new position in said stock spilled too many bean to him so he pitched the idea and they ran with it know it sound like made this up but it true know the company inside and out including catalyst upcoming what would you do all want is an analyst job can believe this guy stole my idea instead of offering me job</th>\n      <td>0.84630</td>\n      <td>-0.26575</td>\n    </tr>\n  </tbody>\n</table>\n<p>344358 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 25 ms\n"
     ]
    }
   ],
   "source": [
    "Vt = pd.DataFrame(svd_fit_transform.round(5),\n",
    "             index = sanitized_posts_df['posts'],\n",
    "             columns = [\"component_1\",\"component_2\" ])\n",
    "Vt"
   ]
  },
  {
   "source": [
    "# TODO Figure out cosine_similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 4.09 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity((topic_word.iloc[0], topic_word.iloc[1])).round()"
   ]
  },
  {
   "source": [
    "# Implementing NMF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "NMF_vectorizer = CountVectorizer(stop_words='english')\n",
    "NMF_posts = NMF_vectorizer.fit_transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(2, random_state=42)\n",
    "NMF_nmf = nmf_model.fit_transform(NMF_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "NMF_df = pd.DataFrame(nmf_model.components_.round(3), index=['component_1', 'component_2'], columns=NMF_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                00    000   0000  00000  000000  00000000  000000000  \\\n",
       "component_1  2.284  1.426  0.003    0.0   0.002       0.0        0.0   \n",
       "component_2  0.000  0.105  0.000    0.0   0.000       0.0        0.0   \n",
       "\n",
       "             000000000000000001  \\\n",
       "component_1                 0.0   \n",
       "component_2                 0.0   \n",
       "\n",
       "             00000000000000f34b57e3bee97172558574a2b2a5d50e20e23b45e28955673f  \\\n",
       "component_1                                                0.0                  \n",
       "component_2                                                0.0                  \n",
       "\n",
       "             0000000000000558  ...  𝗽𝗿𝗶𝗰𝗲  𝘁𝗵𝗮𝗻  𝘁𝗵𝗲  𝘐𝘯𝘵𝘦𝘭𝘭𝘪𝘨𝘦𝘯𝘵  𝘐𝘯𝘷𝘦𝘴𝘵𝘰𝘳  \\\n",
       "component_1               0.0  ...    0.0   0.0  0.0          0.0       0.0   \n",
       "component_2               0.0  ...    0.0   0.0  0.0          0.0       0.0   \n",
       "\n",
       "             𝘛𝘩𝘦  𝚃𝚊𝚗𝚔   𝟐𝟓   𝟙𝟟  𝟚𝟙𝟝ℙ  \n",
       "component_1  0.0   0.0  0.0  0.0   0.0  \n",
       "component_2  0.0   0.0  0.0  0.0   0.0  \n",
       "\n",
       "[2 rows x 284009 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0000</th>\n      <th>00000</th>\n      <th>000000</th>\n      <th>00000000</th>\n      <th>000000000</th>\n      <th>000000000000000001</th>\n      <th>00000000000000f34b57e3bee97172558574a2b2a5d50e20e23b45e28955673f</th>\n      <th>0000000000000558</th>\n      <th>...</th>\n      <th>𝗽𝗿𝗶𝗰𝗲</th>\n      <th>𝘁𝗵𝗮𝗻</th>\n      <th>𝘁𝗵𝗲</th>\n      <th>𝘐𝘯𝘵𝘦𝘭𝘭𝘪𝘨𝘦𝘯𝘵</th>\n      <th>𝘐𝘯𝘷𝘦𝘴𝘵𝘰𝘳</th>\n      <th>𝘛𝘩𝘦</th>\n      <th>𝚃𝚊𝚗𝚔</th>\n      <th>𝟐𝟓</th>\n      <th>𝟙𝟟</th>\n      <th>𝟚𝟙𝟝ℙ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>2.284</td>\n      <td>1.426</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>0.000</td>\n      <td>0.105</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 284009 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "NMF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\namp, http, gt, com, earnings, stock, market, ha, click, company\n\nTopic  1\ncomment, wallstreetbets, 17, 15, spy, 24, 19, 20, www, reddit\ntime: 295 ms\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, NMF_vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=TfidfVectorizer(max_df=0.7, min_df=5,\n                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n                            'itself', ...]).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-22d58dfb8e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for TFIDF DTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlda_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlda_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \"\"\"\n\u001b[1;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         X = self._check_non_neg_array(X, reset_n_features=True,\n\u001b[0m\u001b[1;32m    557\u001b[0m                                       whom=\"LatentDirichletAllocation.fit\")\n\u001b[1;32m    558\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_check_non_neg_array\u001b[0;34m(self, X, reset_n_features, whom)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m         X = self._validate_data(X, reset=reset_n_features,\n\u001b[0m\u001b[1;32m    483\u001b[0m                                 accept_sparse='csr')\n\u001b[1;32m    484\u001b[0m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    613\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=TfidfVectorizer(max_df=0.7, min_df=5,\n                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n                            'itself', ...]).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# for TF DTM\n",
    "# lda_tf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "# lda_tf.fit(tfidfvectorized_posts)\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tfidf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archive/lda.20.0.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(lda_tfidf, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archive/lda.20.0.pkl', 'rb') as picklefile:\n",
    "    lda_tfidf = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_lda = lda_tfidf.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, tfidfvectorized_posts, TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}