{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "# time: 1.16 s\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from lib.Text_Pre_Processing_in_Python import Preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 246 µs\n"
     ]
    }
   ],
   "source": [
    "# time: 235 µs\n",
    "\n",
    "vectorizer_max_features = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 9.26 s\n"
     ]
    }
   ],
   "source": [
    "# time: 6.5 s\n",
    "\n",
    "all_text = pd.read_csv('archive/sub_reddits.csv', low_memory=False)\n",
    "all_text['selftext'] = all_text['selftext'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 22min 24s\n",
    "\n",
    "prep = Preprocess.Preprocess()\n",
    "sanitized_posts = []\n",
    "\n",
    "for post in all_text['selftext']:\n",
    "    document = re.sub(r'https*:\\/\\/[\\w*\\-*\\.]*[\\w\\-*\\/]*[\\.\\w]*[^\\\"\\s]*', ' ', str(post))\n",
    "    document = re.sub(r'\\/[\\w*\\-*\\.]*[\\w\\-*\\/]*[\\.\\w]*[^\\\"\\s]*', ' ', document)\n",
    "    document = re.sub(r'amp;', '', document)\n",
    "    document = \" \".join(prep.preprocess(document))\n",
    "    sanitized_posts.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 191 ms\n",
    "\n",
    "with open('archive/pre_processed.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(sanitized_posts, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 238 ms\n",
    "\n",
    "with open('archive/pre_processed.pkl', 'rb') as picklefile:\n",
    "    sanitized_posts_df = pd.DataFrame(pickle.load(picklefile), columns=[\"posts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "# time: 2min 13s\n",
    "\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != 'test']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != 'a']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '_']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != ' ']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'].str.contains('[^\\d^\\s]')]\n",
    "sanitized_posts_df['posts'] = sanitized_posts_df['posts'].astype(str)\n",
    "\n",
    "sanitized_posts_df.to_csv('archive/sanitized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 703 µs\n",
    "\n",
    "# Workspace for additional text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "# time: 1.53 s\n",
    "\n",
    "sanitized_posts_df = pd.read_csv('archive/sanitized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following section runs a Latent Semantic Analysis (LSA) on the corpus using TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "# time: 21.3 s\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "count_vectorized_posts = count_vectorizer.fit_transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.12226753 0.0818686 ]\n0.20413612240416884\n[2464.32562309 1920.00286489]\ntime: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "# time: 5 s\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "svd_fit_transform = svd.fit_transform(count_vectorized_posts)\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                aa  aaa  aaaaa  aaaaaand  aaaaand  aaaand  aaand  aaarated  \\\n",
       "component_1  0.001  0.0    0.0       0.0      0.0     0.0    0.0       0.0   \n",
       "component_2 -0.000  0.0    0.0      -0.0      0.0     0.0    0.0      -0.0   \n",
       "\n",
       "             aaba  aac  ...   то  уоu  что  это  ісо  الاستثمار   في  كما  \\\n",
       "component_1   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0   \n",
       "component_2   0.0 -0.0  ...  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0   \n",
       "\n",
       "              من  ﬁnancial  \n",
       "component_1  0.0       0.0  \n",
       "component_2  0.0       0.0  \n",
       "\n",
       "[2 rows x 38570 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aaaaa</th>\n      <th>aaaaaand</th>\n      <th>aaaaand</th>\n      <th>aaaand</th>\n      <th>aaand</th>\n      <th>aaarated</th>\n      <th>aaba</th>\n      <th>aac</th>\n      <th>...</th>\n      <th>то</th>\n      <th>уоu</th>\n      <th>что</th>\n      <th>это</th>\n      <th>ісо</th>\n      <th>الاستثمار</th>\n      <th>في</th>\n      <th>كما</th>\n      <th>من</th>\n      <th>ﬁnancial</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 38570 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 55.4 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 63.8 ms\n",
    "\n",
    "topic_word = pd.DataFrame(svd.components_.round(3), index=[\"component_1\", \"component_2\"], columns=count_vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 651 µs\n"
     ]
    }
   ],
   "source": [
    "# time: 478 µs\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ngt, earnings, stock, ha, market\n\nTopic  1\nnigger, america, nt, trade, post\ntime: 30.1 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 37.7 ms\n",
    "\n",
    "display_topics(svd, count_vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    component_1  component_2\n",
       "posts                                                                       \n",
       "sober judge go sleep idea get go write notepad ...      1.59903      0.00004\n",
       "sold first house project house lived renovating...      1.09331      0.00002\n",
       "heard story numerous nonpenny stock sold penny ...      6.48138      0.00001\n",
       "want learn type investing may decide focus stud...      0.60494      0.00001\n",
       "someone recommend good charting site see stock ...      1.30685      0.00000\n",
       "...                                                         ...          ...\n",
       "disclaimer long equity please due diligence bas...      2.90362     -0.00001\n",
       "month ago ran across forum similar reddit peopl...      0.50072      0.00001\n",
       "love researching quality information interestin...      1.42672      0.00001\n",
       "knowing everything know investor could go back ...      0.63715      0.00001\n",
       "contacted lead sell side analyst discus small c...      1.29972      0.00002\n",
       "\n",
       "[335348 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>component_1</th>\n      <th>component_2</th>\n    </tr>\n    <tr>\n      <th>posts</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sober judge go sleep idea get go write notepad good idea get better one submit wrote expected variance time option put call price linear function respect time maximize call putt cost v gross rev combined prob dist stock price profit maybe get something going submit get working program use subreddits like department lol</th>\n      <td>1.59903</td>\n      <td>0.00004</td>\n    </tr>\n    <tr>\n      <th>sold first house project house lived renovating part time past year profit need invest money used part payment next house roughly year couple idea would like hear would situation edit etf large portion investment pro personal thought favorite fund etc</th>\n      <td>1.09331</td>\n      <td>0.00002</td>\n    </tr>\n    <tr>\n      <th>heard story numerous nonpenny stock sold penny level thursday pretty obvious happened us government must instructed largest volume trader institutional trader dump large volume stock penny level right pm eastern time thursday orchestrated effort drive key us stock market index thereby dramatically devalue us stock market objection would nt institution lose combined billion dollar dumping stock penny level yes indeed would believe us federal reserve promised recoup institution whatever loss incurred objection secret china among country ha trying slow economy stop currency quickly devalued hard imagine us would want thing covertly us dollar ha severely devalued recently week one ha wonder firm gained us stock sold penny level suspect could chinese investor word possible china took gigantic windfall profit expense us stock market precisely nt want take one us market manipulation capability assert dominance chinese market manipulation capability know seems counterintuitive doe idea slowing economy order bolster currency would surprised see currency warfare continue month big chinese firm learn fight back thursday proved american innovation leap bound ahead chinese regard market currency manipulation dollar ridiculously stronger today wa day ago euro yuan</th>\n      <td>6.48138</td>\n      <td>0.00001</td>\n    </tr>\n    <tr>\n      <th>want learn type investing may decide focus study certain area specifically shortly lot time hand love read study new thing doe anyone resource share book buy online literature etc</th>\n      <td>0.60494</td>\n      <td>0.00001</td>\n    </tr>\n    <tr>\n      <th>someone recommend good charting site see stock big long list displaying one year chart option would nice preferably moving average rsi etc use yahoo finance clicking stock take way long</th>\n      <td>1.30685</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>disclaimer long equity please due diligence based minute quick analysis company overview xeris spec pharma company founded primary scope work involves developed injectable infusible drug present product launch first product wa approved september called gvoke pfs autoinjector ha glucagon treat severe hypoglycemia market two different product gvoke pfs nov gvoke hypopen july competition primary competition come tradition glucagon kit eli lilly ’ baqsimi baqsimi delivered via nasal passage legacy kit traditional syringe injection xeris ha advantage pfs autoinjector traditionally well received patient xb baqsimi xb legacy kit xb financials popular product likely twopack hypopen autoinjector carry awp adult prescription mg baqsimi cost similar mg dosage legacy kit lly cost xb gvoke micromedex one overhang xeris ’ financials longterm debt ha increased ye q however principal payment start interest expense year xeris generating enough fcf next two year service debt valuation takeaways · peak sale · wacc · positive ebitda xb dcf xb price target</th>\n      <td>2.90362</td>\n      <td>-0.00001</td>\n    </tr>\n    <tr>\n      <th>month ago ran across forum similar reddit people could anonymously publish short thesis unfortunately unable find since doe anyone know still know talking</th>\n      <td>0.50072</td>\n      <td>0.00001</td>\n    </tr>\n    <tr>\n      <th>love researching quality information interesting company however hard find intersection intriguing yet understandable outsider unfortunately rule pharma example really enjoyed following tesla always passionate alternative source energy lowcost airline flying around europe since wa month old love ryanair wizz though nt actually invested two u lowcost airline instead interesting note usually engaging company better ha done financially looking forward tip</th>\n      <td>1.42672</td>\n      <td>0.00001</td>\n    </tr>\n    <tr>\n      <th>knowing everything know investor could go back time give beginner self advice would share comment let spread wisdom everyone</th>\n      <td>0.63715</td>\n      <td>0.00001</td>\n    </tr>\n    <tr>\n      <th>contacted lead sell side analyst discus small cap researching beginning quarter spoke phone favorite fund initiated largest new position said stock spilled many bean pitched idea ran know sound like made true know company inside including catalyst upcoming would want analyst job ca nt believe guy stole idea instead offering job</th>\n      <td>1.29972</td>\n      <td>0.00002</td>\n    </tr>\n  </tbody>\n</table>\n<p>335348 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 25 ms\n",
    "\n",
    "Vt = pd.DataFrame(svd_fit_transform.round(5),\n",
    "             index = sanitized_posts_df['posts'],\n",
    "             columns = [\"component_1\",\"component_2\" ])\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Figure out cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 4.09 ms\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity((topic_word.iloc[0], topic_word.iloc[1])).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 20.3 s\n",
    "\n",
    "NMF_vectorizer = CountVectorizer(stop_words='english')\n",
    "NMF_posts = NMF_vectorizer.fit_transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 8.25 s\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(2, random_state=42)\n",
    "NMF_nmf = nmf_model.fit_transform(NMF_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 382 ms\n",
    "\n",
    "NMF_df = pd.DataFrame(nmf_model.components_.round(3), index=['component_1', 'component_2'], columns=NMF_vectorizer.get_feature_names())\n",
    "\n",
    "NMF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 295 ms\n",
    "\n",
    "display_topics(nmf_model, NMF_vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 17 s\n",
    "\n",
    "LDA_vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english', lowercase=True, token_pattern=r'\\b[a-zA-Z]{3,}\\b', max_df=0.5, min_df=10)\n",
    "\n",
    "LDA_transformed = LDA_vectorizer.fit_transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 17.6 s\n",
    "\n",
    "LDA_Tf_vectorizer = TfidfVectorizer(**LDA_vectorizer.get_params())\n",
    "\n",
    "tfidfvectorized_posts = LDA_Tf_vectorizer.fit_transform(sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 17.3 s\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidfvectorized_posts.toarray(), columns=LDA_vectorizer.get_feature_names())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 39min 22s\n",
    "\n",
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_tf.fit(LDA_transformed)\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_tfidf.fit(tfidfvectorized_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 6.46 ms\n",
    "\n",
    "with open('archive/lda.20.0.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(lda_tfidf, picklefile)\n",
    "with open('archive/lda_tfidf.20.0.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(lda_tfidf, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 4.84 ms\n",
    "\n",
    "with open('archive/lda.20.0.pkl', 'rb') as picklefile:\n",
    "    lda_tf= pickle.load(picklefile)\n",
    "with open('archive/lda_tfidf.20.0.pkl', 'rb') as picklefile:\n",
    "    lda_tfidf = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 56.1 s\n",
    "\n",
    "trans_lda = lda_tfidf.transform(tfidfvectorized_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 188 ms\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 1min 45s\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tf, LDA_transformed, LDA_Tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 54.9 s\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, tfidfvectorized_posts, LDA_Tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 1min 40s\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tf, LDA_transformed, LDA_Tf_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 1min 34s\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tf, LDA_transformed, LDA_Tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 561 ms\n",
    "\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 17.5 s\n",
    "\n",
    "cor_vectorizer = CountVectorizer(max_features=20000, stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", binary=True)\n",
    "\n",
    "cor_posts = cor_vectorizer.fit_transform(sanitized_posts_df['posts'])\n",
    "cor_words = list(np.asarray(cor_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 51.3 s\n",
    "\n",
    "topic_model = ct.Corex(n_hidden=6, words=cor_words, seed=1)\n",
    "topic_model.fit(cor_posts, words=cor_words, docs=sanitized_posts_df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 19.8 ms\n",
    "\n",
    "topics = topic_model.get_topics()\n",
    "for n, topic in enumerate(topics):\n",
    "    topic_words, _ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 227 ms\n",
    "\n",
    "topic_model.get_top_docs(topic=4, n_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 372 ms\n",
    "\n",
    "predictions = pd.DataFrame(topic_model.predict(cor_posts), columns=['topic'+str(i) for i in range(6)])\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 208 ms\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 1min 40s\n",
    "\n",
    "topic_model = ct.Corex(n_hidden=6, words=cor_words,\n",
    "                       max_iter=200, verbose=False, seed=1)\n",
    "\n",
    "topic_model.fit(cor_posts, words=cor_words, docs=sanitized_posts_df['posts'], \n",
    "                anchors=[['options'],\n",
    "                         ['tendies'], \n",
    "                         ['yolo'],\n",
    "                         ['god'], \n",
    "                         ['politics']], anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "wv_vectorizer.fit(all_text['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wv = wv_vectorizer.transform(all_text['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archive/wv_vectorizer.pkl', \"wb\") as picklefile:\n",
    "    pickle.dump(wv_vectorizer, picklefile)\n",
    "\n",
    "with open('archive/wv_vectorized.pkl', \"wb\") as picklefile:\n",
    "    pickle.dump(wv, picklefile)"
   ]
  },
  {
   "source": [
    "# Crashes my machine.\n",
    "wv_array = wv.toarray()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.DataFrame(wv_array, columns=wv_vectorizer.get_feature_names())"
   ]
  },
  {
   "source": [
    "# Doc2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "# dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 2.07 s\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(all_text['selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 297 µs\n",
    "\n",
    "print(data_for_training [:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 2.27 ms\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 1min 14s\n",
    "\n",
    "model.build_vocab(data_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 20min 20s\n",
    "\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 5.2 ms\n",
    "# TODO fix infer_vector list\n",
    "print(model.infer_vector(['violent', 'means', 'to', 'destroy', 'the','organization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archive/gensim_model.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(model, picklefile)"
   ]
  },
  {
   "source": [
    "# PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all of the data and plot it on 2 dimensions\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvp_arr = count_vectorized_posts.toarray()\n",
    "cvp_df = pd.DataFrame(cvp_arr)\n",
    "cvp_df.head()"
   ]
  },
  {
   "source": [
    "# crashes\n",
    "pca.fit(cvp_df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcafeatures_train = pca.transform(all_text['selftext'])"
   ]
  },
  {
   "source": [
    "# KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster(X, km=[], num_clusters=0):\n",
    "    color = 'brgcmyk'\n",
    "    alpha = 0.5\n",
    "    s = 20\n",
    "    \n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0], X[:,1], c=color[0], alpha=alpha, s=s)\n",
    "        \n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i],alpha = alpha,s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}