{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autotime"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["The autotime extension is already loaded. To reload it, use:\n  %reload_ext autotime\ntime: 2.5 ms\n"]}],"source":["import psycopg2 as pg\n","import pandas as pd\n","import pickle\n","import numpy\n","import os\n","import spacy\n","import string\n","import time\n","import tools.functions\n","\n","# import textacy\n","from spacy import attrs\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","from sklearn.base import TransformerMixin\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["texts = {}\n","texts['wsb'] = pd.read_json('archive/wallstreetbets_submission.json', lines=True)\n","texts['insub'] = pd.read_json('archive/investing_submission.json', lines=True)\n","texts['opsub'] = pd.read_json('archive/options_submission.json', lines=True)\n","texts['secan'] = pd.read_json('archive/SecurityAnalysis_submission.json', lines=True)\n","\n","all_text = pd.DataFrame()\n","all_text = all_text.append(texts['insub'])\n","all_text = all_text.append(texts['wsb'])\n","all_text = all_text.append(texts['opsub'])\n","all_text = all_text.append(texts['secan'])\n","\n","all_text['selftext'] = all_text['selftext'].apply(lambda x: x.strip() if type(x) == str else x)\n","all_text = all_text[all_text['selftext'] != \"\"]\n","all_text = all_text[all_text['selftext'] != \"[removed]\"]\n","all_text = all_text[all_text['selftext'] != \"[deleted]\"]\n","all_text = all_text[pd.notna(all_text['selftext'])]\n","\n","all_text.to_csv('sub_reddits.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Postgres info to connect\n","connection_args = {\n","    'host': 'localhost',  # We are connecting to our _local_ version of psql\n","    'dbname': 'myersbriggs',    # DB that we are connecting to\n","    'port': 5432          # port we opened on AWS\n","}\n","\n","connection = pg.connect(**connection_args)  # What is that \"**\" there??\n","cur = connection.cursor()\n","\n","sqlCreateTable = \"create table raw_data (id bigint, type varchar(4), posts text);\"\n","\n","cur.execute(sqlCreateTable)\n","connection.commit()\n","\n","# Didn't work.  Created manually.\n","\n","\n","def make_SQL_string():\n","    columns = [\"post_\" + str(i) + \" text, \" for i in range(58)]\n","\n","    return \"CREATE TABLE posts;\"\n","\n","\n","cur.execute(make_SQL_string())\n","connection.commit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["churchill = pd.read_csv('sources/Churchill/Churchill.csv', sep=\"|\", header=None)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 10.6 s\n"]}],"source":["all_txt = pd.read_csv('archive/sub_reddits.csv')\n","all_txt['selftext'] = all_txt['selftext'].astype(str)"]},{"source":["# Needs additional cleaning"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 565 ms\n"]}],"source":["nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 169 ms\n"]}],"source":["docs = [doc for doc in nlp.pipe(all_txt['selftext'][:2], batch_size=250, n_threads=4)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('archive/text_1.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[0:100000], picklefile)\n","with open('archive/text_2.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[100000:200000], picklefile)\n","with open('archive/text_3.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[200000:300000], picklefile)\n","with open('archive/text_4.pkl', 'wb') as picklefile:\n","    pickle.dump(docs[300000:344748], picklefile)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 10min 50s\n"]}],"source":["with open('archive/text_1.pkl', 'rb') as picklefile:\n","    all_text = pickle.load(picklefile)\n","with open('archive/text_2.pkl', 'rb') as picklefile:\n","    text2 = pickle.load(picklefile)\n","with open('archive/text_3.pkl', 'rb') as picklefile:\n","    text3 = pickle.load(picklefile)\n","with open('archive/text_4.pkl', 'rb') as picklefile:\n","    text4 = pickle.load(picklefile)\n","\n","all_text.extend(text2)\n","all_text.extend(text3)\n","all_text.extend(text4)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 17.7 s\n"]}],"source":["bow_reprs = [doc.count_by(attrs.LOWER) for doc in all_text]\n","\n","vocab_keys = set()\n","for bow_rep in bow_reprs:\n","    vocab_keys.update(set(bow_rep.keys()))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 6.44 s\n"]}],"source":["pos_reprs = [doc.count_by(attrs.POS) for doc in all_text]\n","pos_keys = set()\n","for pos_rep in pos_reprs:\n","    pos_keys.update(set(pos_rep.keys()))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.02 ms\n"]}],"source":["def lexeme_filter(lexeme):\n","    if lexeme.is_digit:\n","        return(False)\n","    if lexeme.is_punct:\n","        return(False)\n","    if lexeme.is_space:\n","        return(False)\n","    if lexeme.like_num:\n","        return(False)\n","    if lexeme.like_email:\n","        return(False)\n","    return(True)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"[E018] Can't retrieve string for hash '17626234509837991929'. This usually refers to an issue with the `Vocab` or `StringStore`.\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-35d8766ae31d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_lexemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_lexemes_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_lexemes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlexeme_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlexeme_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlexeme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlexeme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_lexemes_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrev_lexeme_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexeme_encoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-35d8766ae31d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_lexemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_lexemes_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_lexemes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlexeme_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlexeme_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlexeme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlexeme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_lexemes_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrev_lexeme_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexeme_encoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.__getitem__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mlexeme.pyx\u001b[0m in \u001b[0;36mspacy.lexeme.Lexeme.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[0;34m()\u001b[0m\n","\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '17626234509837991929'. This usually refers to an issue with the `Vocab` or `StringStore`.\""]}],"source":["vocab_lexemes = [nlp.vocab[vk] for vk in vocab_keys]\n","vocab_lexemes_filtered = [vl for vl in vocab_lexemes if lexeme_filter(vl)]\n","\n","lexeme_encoding = {lexeme.lower : i for i,lexeme in enumerate(vocab_lexemes_filtered)}\n","rev_lexeme_encoding = {i:k for k,i in lexeme_encoding.items()}\n","lexeme_word_lookup = {lexeme.lower : lexeme.lower_ for lexeme in vocab_lexemes_filtered}\n","n_words = len(lexeme_encoding)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lexeme_lower_bow_to_vec(lexeme_lower_bow, lexeme_encoding):\n","    bow_vec = numpy.zeros((len(lexeme_encoding,)), dtype=numpy.int64)\n","    for k,v in lexeme_lower_bow.items():\n","        try:\n","            bow_vec[lexeme_encoding[k]] += v\n","        except KeyError:\n","            pass\n","    return(bow_vec)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm = numpy.vstack(lexeme_lower_bow_to_vec(bow_rep, lexeme_encoding)\n","                   for bow_rep in bow_reprs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_df = pd.DataFrame([key for key in lexeme_encoding])\n","lexeme_df.columns = ['key']\n","lexeme_df['col_headers'] = lexeme_df['key'].map(lexeme_encoding)\n","lexeme_df['key_words'] = lexeme_df['key'].map(lexeme_word_lookup)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_df = pd.DataFrame([key for key in lexeme_encoding])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lexeme_encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm_df = pd.DataFrame(tdm)\n","tdm_df.columns = lexeme_df['key_words']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdm_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"orig_nbformat":2,"kernelspec":{"name":"Python 3.8.5 64-bit ('metis': conda)","display_name":"Python 3.8.5 64-bit ('metis': conda)","metadata":{"interpreter":{"hash":"1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"}}}}}