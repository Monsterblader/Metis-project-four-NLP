{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "# time: 1.16 s\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from lib.Text_Pre_Processing_in_Python import Preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 277 µs\n"
     ]
    }
   ],
   "source": [
    "# time: 235 µs\n",
    "\n",
    "vectorizer_max_features = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "# time: 6.5 s\n",
    "\n",
    "all_text = pd.read_csv('archive/sub_reddits.csv', low_memory=False)\n",
    "all_text['selftext'] = all_text['selftext'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18min 45s\n"
     ]
    }
   ],
   "source": [
    "# time: 22min 24s\n",
    "\n",
    "prep = Preprocess.Preprocess()\n",
    "sanitized_posts = []\n",
    "\n",
    "for post in all_text['selftext']:\n",
    "    document = re.sub(r'https*:\\/\\/[\\w*\\-*\\.]*[\\w\\-*\\/]*[\\.\\w]*[^\\\"\\s]*', ' ', str(post))\n",
    "    document = re.sub(r'\\/[\\w*\\-*\\.]*[\\w\\-*\\/]*[\\.\\w]*[^\\\"\\s]*', ' ', document)\n",
    "    document = re.sub(r'amp;', '', document)\n",
    "    document = \" \".join(prep.preprocess(document))\n",
    "    sanitized_posts.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 191 ms\n",
    "\n",
    "with open('archive/pre_processed.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(sanitized_posts, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 238 ms\n",
    "\n",
    "with open('archive/pre_processed.pkl', 'rb') as picklefile:\n",
    "    sanitized_posts_df = pd.DataFrame(pickle.load(picklefile), columns=[\"posts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "# time: 3.05 s\n",
    "\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != 'test']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != 'a']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != '_']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'] != ' ']\n",
    "sanitized_posts_df = sanitized_posts_df[sanitized_posts_df['posts'].str.contains('[^\\d^\\s]')]\n",
    "sanitized_posts_df['posts'] = sanitized_posts_df['posts'].astype(str)\n",
    "\n",
    "sanitized_posts_df.to_csv('archive/sanitized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 703 µs\n",
    "\n",
    "# Workspace for additional text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 941 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 1.53 s\n",
    "\n",
    "sanitized_posts_df = pd.read_csv('archive/sanitized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 520 µs\n"
     ]
    }
   ],
   "source": [
    "# time: 556 µs\n",
    "\n",
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster(X, km=[], num_clusters=0):\n",
    "    color = 'brgcmyk'\n",
    "    alpha = 0.5\n",
    "    s = 20\n",
    "    \n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0], X[:,1], c=color[0], alpha=alpha, s=s)\n",
    "        \n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i],alpha = alpha,s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33 s\n"
     ]
    }
   ],
   "source": [
    "# time: 38.8 s\n",
    "\n",
    "vec = TfidfVectorizer(stop_words=\"english\")\n",
    "vec.fit(all_text.selftext.values)\n",
    "features = vec.transform(all_text.selftext.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(n_clusters=5, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28 s\n"
     ]
    }
   ],
   "source": [
    "# time: 28.9 s\n",
    "\n",
    "cls = MiniBatchKMeans(n_clusters=5, random_state=0)\n",
    "cls.fit(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 4, 4, 4], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.61 s\n"
     ]
    }
   ],
   "source": [
    "# time: 9.15 s\n",
    "\n",
    "# predict cluster labels for new dataset\n",
    "cls.predict(features)\n",
    "\n",
    "# to get cluster labels for the dataset used while\n",
    "# training the model (used for models that does not\n",
    "# support prediction on new dataset).\n",
    "cls.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 346 µs\n"
     ]
    }
   ],
   "source": [
    "random_state = 0\n",
    "# reduce the features to 2D\n",
    "pca = PCA(n_components=2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_features = pca.fit_transform(features.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reduce the cluster centers to 2D\n",
    "reduced_cluster_centers = pca.transform(cls.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(features))\n",
    "plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
